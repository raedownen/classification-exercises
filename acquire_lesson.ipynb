{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification:  Acquire data\n",
    "\n",
    "## Goals\n",
    "\n",
    "Data you wish to use in analysis will be stored in a variety of sources. In this lesson, we will review importing data from a csv and via mySQL, and we will also learn how to import data from our local clipboard, a google sheets document, and from an MS Excel file. \n",
    "We will then select one source to use as we continue through the rest of this module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methods of Data Acquisition\n",
    "\n",
    "\n",
    "- `read_clipboard`: When you have data copied to your clipboard, you can use pandas to read it into a data frame with `pd.read_clipboard`. This can be useful for quickly transferring data to/from a spreadsheet.   \n",
    "- `read_excel`: This function can be used to create a data frame based on the contents of an Excel spreadsheet.  \n",
    "- `read_csv`: Read from a local csv, or from a the cloud (Google Sheets or AWS S3).    \n",
    "- `read_sql(sql_query, connection_url)`: Read data using a SQL query to a database. You must have the required drivers installed, and a specially formatted url string must be provided.\n",
    "    \n",
    "    ```\n",
    "    # To talk to a mysql database:\n",
    "    python -m pip install pymysql mysql-connector\n",
    "    # the connection url string:\n",
    "    mysql+pymysql://USER:PASSWORD@HOST/DATABASE_NAME\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>https://raw.githubusercontent.com/CodeupClassroom/mirzakhani-classification-exercises/main/acquire_lesson.ipynb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [https://raw.githubusercontent.com/CodeupClassroom/mirzakhani-classification-exercises/main/acquire_lesson.ipynb]\n",
       "Index: []"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reads data copied to your clipboard\n",
    "df1 = pd.read_clipboard()\n",
    "df1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source: A Shared Google Sheet\n",
    "\n",
    "1. Get the shareable link url: https://docs.google.com/spreadsheets/d/BLAHBLAHBLAH/edit#gid=NUMBER\n",
    "\n",
    "2. Turn that into a CSV export URL: Replace `/edit` with `/export`; Add `format=csv` to the beginning of the query string. https://docs.google.com/spreadsheets/d/BLAHBLAHBLAH/export?format=csv&gid=NUMBER:\n",
    "\n",
    "3. Pass it to pd.read_csv, which can take a URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://docs.google.com/spreadsheets/d/1Uhtml8KY19LILuZsrDtlsHHDC9wuDGUSe8LTEwvdI5g/export?format=csv&gid=341089357'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sheet_url = 'https://docs.google.com/spreadsheets/d/1Uhtml8KY19LILuZsrDtlsHHDC9wuDGUSe8LTEwvdI5g/edit#gid=341089357'    \n",
    "\n",
    "csv_export_url = sheet_url.replace('/edit#gid=', '/export?format=csv&gid=')\n",
    "csv_export_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>PassengerId</th>\n",
       "      <th>Survived</th>\n",
       "      <th>Pclass</th>\n",
       "      <th>Name</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>SibSp</th>\n",
       "      <th>Parch</th>\n",
       "      <th>Ticket</th>\n",
       "      <th>Fare</th>\n",
       "      <th>Cabin</th>\n",
       "      <th>Embarked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Braund, Mr. Owen Harris</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>A/5 21171</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Cumings, Mrs. John Bradley (Florence Briggs Th...</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>PC 17599</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C85</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>Heikkinen, Miss. Laina</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>STON/O2. 3101282</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>Futrelle, Mrs. Jacques Heath (Lily May Peel)</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>113803</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>C123</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>Allen, Mr. William Henry</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>373450</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>NaN</td>\n",
       "      <td>S</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   PassengerId  Survived  Pclass  \\\n",
       "0            1         0       3   \n",
       "1            2         1       1   \n",
       "2            3         1       3   \n",
       "3            4         1       1   \n",
       "4            5         0       3   \n",
       "\n",
       "                                                Name     Sex   Age  SibSp  \\\n",
       "0                            Braund, Mr. Owen Harris    male  22.0      1   \n",
       "1  Cumings, Mrs. John Bradley (Florence Briggs Th...  female  38.0      1   \n",
       "2                             Heikkinen, Miss. Laina  female  26.0      0   \n",
       "3       Futrelle, Mrs. Jacques Heath (Lily May Peel)  female  35.0      1   \n",
       "4                           Allen, Mr. William Henry    male  35.0      0   \n",
       "\n",
       "   Parch            Ticket     Fare Cabin Embarked  \n",
       "0      0         A/5 21171   7.2500   NaN        S  \n",
       "1      0          PC 17599  71.2833   C85        C  \n",
       "2      0  STON/O2. 3101282   7.9250   NaN        S  \n",
       "3      0            113803  53.1000  C123        S  \n",
       "4      0            373450   8.0500   NaN        S  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_googlesheet = pd.read_csv(csv_export_url)\n",
    "df_googlesheet.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CSV (Hosted or Local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>flavor</th>\n",
       "      <th>pints</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>moolenium crunch</td>\n",
       "      <td>11.057570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>bubblegum</td>\n",
       "      <td>6.288724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>chubby hubby</td>\n",
       "      <td>7.660815</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>bubblegum</td>\n",
       "      <td>6.644338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>neopolitan</td>\n",
       "      <td>13.600125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             flavor      pints\n",
       "0  moolenium crunch  11.057570\n",
       "1         bubblegum   6.288724\n",
       "2      chubby hubby   7.660815\n",
       "3         bubblegum   6.644338\n",
       "4        neopolitan  13.600125"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If the S3 file is private, you will need your S3 configurations setup properly.\n",
    "url = \"https://gist.githubusercontent.com/ryanorsinger/bec2f59a9cef8ae7428cb70b3541354a/raw/ef64298da52e5d70f4d388f5fd48eccdb02ed3f1/ice_cream.csv\"\n",
    "\n",
    "df = pd.read_csv(url)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source: SQL\n",
    "\n",
    "Create a dataframe from the passengers table in the mySQL database, titanic_db. \n",
    "\n",
    "!!!warning \"Database Credentials\"\n",
    "    It's a bad idea to store your database access credentials (i.e. your username and password) in plaintext in your source code. There are many different ways one could manage secrets like this, but a simple way is to store the values in a python file that is not included along with the rest of your source code. This is what we have done with the `env` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = env.get_db_url('titanic_db')\n",
    "\n",
    "df = pd.read_sql('SELECT * FROM passengers', url)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a function that we can reference later to acquire the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_titanic_data():\n",
    "    url = env.get_db_url('titanic_db')\n",
    "    \n",
    "    return pd.read_sql('SELECT * FROM passengers', url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# acquire new data:\n",
    "new_titanic_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll store this function in a file named `acquire.py`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Caching Your Data\n",
    "Because data acquisition can take time, it's a common practice to write the data locally to a `.csv` file. \n",
    "\n",
    "1. Do whatever you need to do to produce the dataframe that you need. \n",
    "    - For example `df = pd.read_sql('SELECT * FROM passengers', get_connection('titanic_db'))`\n",
    "    - Or your dataframe could include joins, multiple data sources, etc...\n",
    "    \n",
    "2. Now use `df.to_csv(\"titanic.csv\")` to write that dataframe to the file.\n",
    "3. Now that you've written the csv file, you can use it later in other parts of your pipeline!\n",
    "4. Consider the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def get_titanic_data():\n",
    "    filename = \"titanic.csv\"\n",
    "    \n",
    "    # if file is available locally, read it\n",
    "    if os.path.isfile(filename):\n",
    "        return pd.read_csv(filename)\n",
    "    \n",
    "    # if file not available locally, acquire data from SQL database\n",
    "    # and write it as csv locally for future use\n",
    "    else:\n",
    "        # read the SQL query into a dataframe\n",
    "        df = new_titanic_data()\n",
    "        \n",
    "        # Write that dataframe to disk for later. Called \"caching\" the data for later.\n",
    "        df.to_csv(filename)\n",
    "\n",
    "        # Return the dataframe to the calling code\n",
    "        return df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_titanic_data()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises\n",
    "\n",
    "The end product of these exercise is a jupyter notebook (`classification_exercises.ipynb`) and a `acquire.py` file. The notebook will contain all your work as you move through the exercises. The `acquire.py` file should contain the final functions that acquire the data into a pandas dataframe.\n",
    "\n",
    "1. Make a new repo called `classification-exercises` on both GitHub and within your `codeup-data-science` directory. This will be where you do your work for this module. \n",
    "\n",
    "2. Inside of your local `classification-exercises` repo, create a file named `.gitignore` and list the following file names and paths: `env.py`, `.DS_Store`, `.ipynb_checkpoints/`, `__pycache__`, `titanic.csv`, `iris.csv`, and `telco.csv`, `*.csv`. Add and commit your `.gitignore` file before moving forward.\n",
    "    \n",
    "3. Now that you are 100% sure that your `.gitignore` file lists `env.py`, create or copy your `env.py` file inside of `classification-exercises`. Running `git status` should show that git is ignoring this file.\n",
    "\n",
    "4. In a jupyter notebook, `classification_exercises.ipynb`, use a python module (pydata or seaborn datasets) containing datasets as a source from the iris data. Create a pandas dataframe, `df_iris`, from this data.\n",
    "\n",
    "    - print the first 3 rows\n",
    "    - print the number of rows and columns (shape)\n",
    "    - print the column names\n",
    "    - print the data type of each column\n",
    "    - print the summary statistics for each of the numeric variables. Would you\n",
    "      recommend rescaling the data based on these statistics?\n",
    "\n",
    "\n",
    "5. Read the `Table1_CustDetails` table from the `Excel_Exercises.xlsx` file into a\n",
    "   dataframe named `df_excel`.\n",
    "\n",
    "    - assign the first 100 rows to a new dataframe, `df_excel_sample`\n",
    "    - print the number of rows of your original dataframe\n",
    "    - print the first 5 column names\n",
    "    - print the column names that have a data type of `object`\n",
    "    - compute the range for each of the numeric variables.\n",
    "\n",
    "\n",
    "6. Read the data from [this google sheet](https://docs.google.com/spreadsheets/d/1Uhtml8KY19LILuZsrDtlsHHDC9wuDGUSe8LTEwvdI5g/edit?usp=sharing) into a dataframe, `df_google`\n",
    "\n",
    "    - print the first 3 rows\n",
    "    - print the number of rows and columns\n",
    "    - print the column names\n",
    "    - print the data type of each column\n",
    "    - print the summary statistics for each of the numeric variables\n",
    "    - print the unique values for each of your categorical variables\n",
    "\n",
    "\n",
    "Make a new python module, `acquire.py` to hold the following data aquisition functions:\n",
    "\n",
    "7. Make a function named `get_titanic_data` that returns the titanic data from the codeup data science database as a pandas data frame. Obtain your data from the _Codeup Data Science Database_. \n",
    "\n",
    "\n",
    "8. Make a function named `get_iris_data` that returns the data from the `iris_db` on the codeup data science database as a pandas data frame. The returned data frame should include the actual name of the species in addition to the `species_id`s. Obtain your data from the _Codeup Data Science Database_. \n",
    "\n",
    "9. Make a function named `get_telco_data` that returns the data from the `telco_churn` database in SQL. In your SQL, be sure to join all 4 tables together, so that the resulting dataframe contains all the contract, payment, and internet service options. Obtain your data from the _Codeup Data Science Database_. \n",
    "\n",
    "10. Once you've got your `get_titanic_data`, `get_iris_data`, and `get_telco_data` functions written, now it's time to add caching to them. To do this, edit the beginning of the function to check for the local filename of `telco.csv`, `titanic.csv`, or `iris.csv`. If they exist, use the .csv file. If the file doesn't exist, then produce the SQL and pandas necessary to create a dataframe, then write the dataframe to a .csv file with the appropriate name. \n",
    "\n",
    "__Be sure to add env.py, titanic.csv, iris.csv, and telco.csv to your .gitignore file__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": false,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": false,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
